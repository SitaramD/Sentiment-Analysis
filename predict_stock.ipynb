{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all files from the links saved in a txt file\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "urls=[]\n",
    "dest_Path='destination'\n",
    "\n",
    "with open('open sourcefile') as f:\n",
    "    for line in f:\n",
    "        urls.append(line)\n",
    "\n",
    "    for link in urls:\n",
    "            link = link.strip()\n",
    "            name = link.rsplit('/', 1)[-1]\n",
    "            filename = os.path.join(dest_Path, name)\n",
    "            \n",
    "            if not os.path.isfile(filename):\n",
    "                print('Downloading: ' + filename)\n",
    "            try:\n",
    "                urllib.request.urlretrieve(link, filename)\n",
    "            except Exception as inst:\n",
    "                print(inst)\n",
    "                print('  Encountered unknown error. Continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read data from csv_file and complete cleaning taks on it\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path\n",
    "import os,json\n",
    "import pandas.api.types as ptypes\n",
    "import nltk\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def getDataFromJsonToCsv(pathFrom,pathTo):\n",
    "\n",
    "    # set path to file\n",
    "    p = Path(pathFrom)\n",
    "    #save data from json file\n",
    "    data=[]\n",
    "\n",
    "    \n",
    "    rem = string.punctuation\n",
    "    rem=rem.translate({ord('_'): None})\n",
    "    pattern = r\"[{}]\".format(rem)\n",
    "    \n",
    "    # read json and save in data variable\n",
    "    with p.open('r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    # create and normalize dataframe\n",
    "    df = json_normalize(data,'entities',['date',\"url\",\"lang\"],record_prefix='')\n",
    "    \n",
    "    url_values = df[(df['name'].str.lower().str.contains('facebook|economy|amazon|microsoft|google|apple')) & (df['avgSalience']>0.04)].url.unique()\n",
    "\n",
    "    df=df[df.url.isin(url_values)]\n",
    "    \n",
    "    #Change whitespace into an underscore, using numpy to optimize the scaling\n",
    "    df['name']=np.core.defchararray.replace(df['name'].values.astype(str), ' ', '_')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Group date by url/date and sum the single rows of name/numMentions in single list rows\n",
    "    df=df.groupby(['url','date'], as_index=False).agg({\n",
    "    'name': lambda x: list(x),\n",
    "    'numMentions': lambda x: list(x),\n",
    "    })\n",
    "    \n",
    "    #Nehme die Liste auseinander und ordne sie in reihen\n",
    "    series = [df[i].explode() for i in ['name','numMentions']]\n",
    "    #multipliziere die jeweilihen miteinander und fÃ¼ge sie in die Liste\n",
    "    df['name'] = series[0].str.lower().repeat(series[1]).groupby(level=0).agg(list)\n",
    "    \n",
    "    #digits into blank\n",
    "    series_one = [df[i].explode() for i in ['name']]\n",
    "    df['name']=series_one[0].str.replace('\\w*\\d\\w*','').groupby(level=0).agg(list)\n",
    "    \n",
    "    #punctation into blank\n",
    "    series_two = [df[i].explode() for i in ['name']]\n",
    "    df['name']=series_two[0].str.replace(pattern,'').groupby(level=0).agg(list)\n",
    "    \n",
    "    df.to_csv(pathTo)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot table of Gdelt Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2)) \n",
    "ax.xaxis.set_visible(False)  \n",
    "ax.yaxis.set_visible(False)  \n",
    "ax.set_frame_on(False)  k\n",
    "table = table(ax, data, loc='upper right')  \n",
    "table.auto_set_font_size(False) \n",
    "table.set_fontsize(12) \n",
    "table.scale(4, 4) \n",
    "table.auto_set_column_width(col=list(range(len(data.columns))))\n",
    "plt.savefig(\"save_to.png\",dpi=800, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the dataFrame to explore it easier \n",
    "\n",
    "#Vectorize the corpus with sklearn Countvectorizer\n",
    "def Countvectorize_corpus(data):\n",
    "  \n",
    "    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    bag_of_words = vectorizer.fit_transform(data['name'])\n",
    "    X=bag_of_words.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "#wÃ¶rter suchen\n",
    "def search_word(vec,bag,word):\n",
    "    i = vec.get_feature_names().index(word)\n",
    "    return bag[:,i].nonzero()[0]\n",
    "#search_word(vectorizer,bag_of_words,'linear')\n",
    "\n",
    "#merge original dataframe with vectorized one\n",
    "def Vectorized_dataframe(data):\n",
    "    data['date']=pd.to_datetime(data['date'].astype(float), unit='s')\n",
    "    data = data.merge(test, left_index=True, right_index=True, how='left')\n",
    "    data.drop(['name_x','numMentions'],axis=1)\n",
    "    return dd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Countvectorize_corpus(master_file.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all downloaded json Files, convert them into csv_files and concat all files into one master file\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "def queryFiles(data):\n",
    "    \n",
    "    json_files=[]\n",
    "    csv_files=[]\n",
    "    count=1615\n",
    "    #read all json files in a directonary\n",
    "    json_files = [pos_json for pos_json in os.listdir(data) if pos_json.endswith('json')]\n",
    "    \n",
    "    #if there were any readable json files, optimize and convert it to csv\n",
    "    if len(json_files)>0:\n",
    "        for i in range(len(json_files)):\n",
    "            csv_files.append(getDataFromJsonToCsv(data+json_files[i],'sourcefile//file{}.csv'.format(count)))\n",
    "            count+=1\n",
    "            \n",
    "       \n",
    "    new_df= pd.concat(csv_files)\n",
    "    \n",
    "    return new_df.to_csv(\"save_to\")\n",
    "      \n",
    "queryFiles('jsonFiles') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create own Lexicon based on LM and Minqing Hu and Bing Liu lexicons by splitting the words into positve/negative lists\n",
    "\n",
    "from fractions import Fraction\n",
    "lm_master_dict=pd.read_csv('loghranmcdonald dictionary')\n",
    "positive_words=[]\n",
    "negative_words=[]\n",
    "\n",
    "\n",
    "n_frame=lm_master_dict[lm_master_dict['Negative']>0]\n",
    "p_frame=lm_master_dict[lm_master_dict['Positive']>0]\n",
    "\n",
    "positive_words=[x for x in p_frame['Word']]\n",
    "negative_words=[x for x in n_frame['Word']]\n",
    "\n",
    "negative=0\n",
    "positive=0\n",
    "result=0\n",
    "\n",
    "#Opinion Lexicon from Minqing Hu and Bing Liu\n",
    "with open(\"lui dictionary\") as f:\n",
    "    for line in f:\n",
    "        newline = line.rstrip('\\n')\n",
    "        if newline.upper() not in positive_words:\n",
    "            positive_words.append(newline.upper())\n",
    "\n",
    "len(positive_words),len(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train own lexicon on labelled data based on Full-Economic-News-DFE-83986 and Headline Data\n",
    "#Normalize and cleane the labelled data, fill empty rows with values to avoid exceptions\n",
    "\n",
    "import ast\n",
    "from fractions import Fraction\n",
    "from itertools import zip_longest\n",
    "\n",
    "p=Path(\"Headline_Trainingdata.json\")\n",
    "news=pd.read_json(p)\n",
    "\n",
    "mmm=pd.read_csv('Full-Economic-News-DFE-839861.csv')\n",
    "mmm[mmm['positivity']>0]\n",
    "mmm=mmm.loc[:, mmm.columns.intersection(['positivity','headline','text'])]\n",
    "mmm[mmm['positivity']>4].iloc[0][2]\n",
    "\n",
    "news=news.append(mmm[mmm['positivity']>0], ignore_index=True)\n",
    "news[news['positivity'].notnull()]['text'].iloc[0]\n",
    "\n",
    "values={'company':'zero','headline':'zero','positivity':0,'text':'zero','title':'zero','sentiment':0}\n",
    "news=news.fillna(value=values)\n",
    "news=news.drop('id', axis=1)\n",
    "news.loc[(news.positivity >0) | (news.sentiment !=0)]\n",
    "news['text']=news['text'].apply(cleanhtml)\n",
    "news['text']=news['text'].apply(str.lower)\n",
    "news['tokenized_text']=news['text'].apply(nltk.sent_tokenize)\n",
    "\n",
    "positive_news=news.loc[(news.positivity >4) | (news.sentiment >0)]\n",
    "positive_news=positive_news[['title','text','tokenized_text']]\n",
    "\n",
    "negative_news=news.loc[((news.positivity >0) & (news.positivity <5) ) | (news.sentiment <0)]\n",
    "negative_news=negative_news[['title','text','tokenized_text']]\n",
    "\n",
    "\n",
    "\n",
    "positive_dic={}\n",
    "negative_dic={}\n",
    "\n",
    "\n",
    " \n",
    "#According to the formula of \n",
    "for i in range(len(positive_words)):\n",
    "    result_positive=positive_news.loc[(positive_news['title'].str.contains(positive_words[i].lower())) | (positive_news['text'].str.contains(positive_words[i].lower()))]\n",
    "    result_negative=negative_news.loc[(negative_news['title'].str.contains(positive_words[i].lower())) | (negative_news['text'].str.contains(positive_words[i].lower()))]\n",
    "   \n",
    "    if result_positive.empty is False:\n",
    "        positive_dic[i]=(positive_words[i],float(Fraction(len(result_positive),len(result_positive)+len(result_negative))),float(Fraction(len(result_negative),len(result_positive)+len(result_negative))))\n",
    "\n",
    "\n",
    "for i in range(len(negative_words)):\n",
    "    result_positive=positive_news.loc[(positive_news['title'].str.contains(negative_words[i].lower())) | (positive_news['text'].str.contains(negative_words[i].lower()))]\n",
    "    result_negative=negative_news.loc[(negative_news['title'].str.contains(negative_words[i].lower())) | (negative_news['text'].str.contains(negative_words[i].lower()))]\n",
    "    \n",
    "    if result_negative.empty is False:\n",
    "        negative_dic[i]=(negative_words[i],float(Fraction(len(result_positive),len(result_positive)+len(result_negative))),float(Fraction(len(result_negative),len(result_positive)+len(result_negative))))\n",
    "\n",
    "len(positive_dic),len(negative_dic)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2)) \n",
    "ax.xaxis.set_visible(False)  \n",
    "ax.yaxis.set_visible(False)  \n",
    "ax.set_frame_on(False) \n",
    "table = table(ax, headline, loc='upper right')  \n",
    "table.auto_set_font_size(False) \n",
    "table.set_fontsize(12) \n",
    "table.scale(2, 2) \n",
    "table.auto_set_column_width(col=list(range(len(headline.columns))))\n",
    "plt.savefig(\"headlineDataSet.png\",dpi=400, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a polarity score according to the formula in the following paper:\n",
    "#Change structure of dicts into DataFrame for better visualization \n",
    "#This step is only necessary for visualization, the calculating of polarity_score can be done in the previous dictionary structure too\n",
    "\n",
    "\n",
    "#CREATING DataFrame FOR POSITIVE WORDS\n",
    "df=pd.DataFrame(positive_dic)\n",
    "df=df.rename(columns=df.iloc[0])\n",
    "df_positive=df.iloc[1:]\n",
    "df_positive.loc[3]=2*(df_positive.iloc[0])-1\n",
    "\n",
    "#Change STRUCTURE OF DATAFRAME and column names for better visualization\n",
    "df_positive=df_positive.melt()\n",
    "df_positive = df_positive.groupby('variable')['value'].apply(list).reset_index()       # groupby to list\n",
    "df_positive = df_positive.join(pd.DataFrame(df_positive.pop('value').values.tolist()))  # expand lists to columns\n",
    "df_positive=df_positive.rename(columns={'variable':'Word',0:'positiveSentScore',1:'negativeSentScore',2:'polarityScore'})\n",
    "df_positive\n",
    "\n",
    "\n",
    "#CREATING DataFrame FOR NEGATIVE WORDS\n",
    "df_negative=pd.DataFrame(negative_dic)\n",
    "df_negative=df_negative.rename(columns=df_negative.iloc[0])\n",
    "df_negative=df_negative.iloc[1:]\n",
    "df_negative.loc[3]=2*(df_negative.iloc[0])-1\n",
    "\n",
    "#Change STRUCTURE OF DATAFRAME and column names for better visualization\n",
    "df_negative=df_negative.melt()\n",
    "df_negative = df_negative.groupby('variable')['value'].apply(list).reset_index()       # groupby to list\n",
    "df_negative = df_negative.join(pd.DataFrame(df_negative.pop('value').values.tolist()))  # expand lists to columns\n",
    "df_negative=df_negative.rename(columns={'variable':'Word',0:'positiveSentScore',1:'negativeSentScore',2:'polarityScore'})\n",
    "df_negative,df_positive\n",
    "\n",
    "\n",
    "#Combine both dataframes\n",
    "\n",
    "df_own_lexicon = pd.concat([df_positive,df_negative])\n",
    "df_own_lexicon.to_csv(\"/OwnLexiconPolarity.csv\",index=False)\n",
    "df_own_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine polarity_scores for the LM lexicon\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "postive_dict_LM={}\n",
    "negative_dict_LM={}\n",
    "\n",
    "for i in range(len(positive_words)):\n",
    "    postive_dict_LM[i]=(positive_words[i],1)\n",
    "    \n",
    "for i in range(len(negative_words)):\n",
    "    negative_dict_LM[i]=(negative_words[i],-1)\n",
    "\n",
    "#Combine both dictionaries\n",
    "\n",
    "#CREATING DataFrame FOR POSITIVE WORDS\n",
    "dff=pd.DataFrame(postive_dict_LM)\n",
    "dff=dff.rename(columns=dff.iloc[0])\n",
    "dff_positive=dff.iloc[1:]\n",
    "dff_positive\n",
    "\n",
    "#Change STRUCTURE OF DATAFRAME and column names for better visualization\n",
    "dff_positive=dff_positive.melt()\n",
    "dff_positive=dff_positive.rename(columns={'variable':'Word','value':'polarityScore'})\n",
    "dff_positive\n",
    "\n",
    "\n",
    "#CREATING DataFrame FOR NEGATIVE WORDS\n",
    "dfff=pd.DataFrame(negative_dict_LM)\n",
    "dfff=dfff.rename(columns=dfff.iloc[0])\n",
    "dfff_positive=dfff.iloc[1:]\n",
    "dfff_positive\n",
    "\n",
    "#Change STRUCTURE OF DATAFRAME and column names for better visualization\n",
    "dfff_positive=dfff_positive.melt()\n",
    "dfff_positive=dfff_positive.rename(columns={'variable':'Word','value':'polarityScore'})\n",
    "dfff_positive\n",
    "\n",
    "\n",
    "df_LM_lexicon = pd.concat([dff_positive,dfff_positive])\n",
    "df_LM_lexicon.to_csv(\"LmLexiconPolarityNew.csv\",index=False)\n",
    "df_LM_lexicon\n",
    "df_LM_lexicon['polarityScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "own_dict=dict(zip(df_own_lexicon.Word.str.lower(), df_own_lexicon.polarityScore))\n",
    "own_dict\n",
    "\n",
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "LM_dict=dict(zip(df_LM_lexicon.Word.str.lower(), df_LM_lexicon.polarityScore))\n",
    "len(LM_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label the whole dataset according the chosen dictionary we created previously\n",
    "#First method is for labelling single rows, second one is for all rows in a DataFrame\n",
    "#Both method are explicitly created for LM and own created dictionaries\n",
    "\n",
    "#label single Data in negative/positive polarity\n",
    "def labelData(data,dictionary):\n",
    "    polarity_result=0\n",
    "    pol_list=[]\n",
    "    for i in data:\n",
    "        if dictionary.get(i) is not None :\n",
    "            print(i,dictionary.get(i))\n",
    "            pol_list.append(dictionary.get(i))\n",
    "            polarity_result=sum(pol_list)/len(pol_list)\n",
    "            \n",
    "    return polarity_result    \n",
    "\n",
    "#label all rows in a DataFrame into negative/positive DataFrames\n",
    "def labelDataSet(data,dictionary,pathTo):\n",
    "    row_counter=0\n",
    "    polarity_result=0\n",
    "    pol_list=[]\n",
    "    while row_counter <len(data['name']): \n",
    "        for i in data['name'][row_counter]:\n",
    "            if dictionary.get(i) is not None:\n",
    "                pol_list.append(dictionary.get(i))\n",
    "                polarity_result=sum(pol_list)/len(pol_list)\n",
    "                data['numMentions'][row_counter]=polarity_result\n",
    "        \n",
    "        row_counter+=1\n",
    "    return data.to_csv(pathTo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentimentanalysis with SentWordNet, our third dictionary\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "    \n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def getSentimentWithSentiWordNet(data):\n",
    "    words=data\n",
    "    words=' '.join(words)\n",
    "    tokens=word_tokenize(words)\n",
    "    tags = pos_tag(tokens)\n",
    "    sentiment = 0\n",
    "    count = 0\n",
    "\n",
    "    for word, tag in tags:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(word, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        word_sent = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            \n",
    "        if word_sent !=0:\n",
    "            sentiment += word_sent\n",
    "            count += 1\n",
    "                \n",
    "    if count >0:\n",
    "        sentiment = sentiment/count\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "#label all rows in a DataFrame into negative/positive DataFrames\n",
    "def labelDataSentiWordNet(data,pathTo):\n",
    "    row_counter=0\n",
    "    polarity_result=0\n",
    "    while row_counter <len(data['NameSet']): \n",
    "        polarity_result=getSentimentWithSentiWordNet(data['NameSet'][row_counter])\n",
    "        data['numMentions'][row_counter]=polarity_result\n",
    "        row_counter+=1\n",
    "        print(f' Number {row_counter}')\n",
    "    return data.to_csv(pathTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facebook\n",
    "COLORS = [\"red\", \"orange\", \"green\"]\n",
    "\n",
    "# Drop the columns that aren't useful for the plot\n",
    "plot_day = facebook.drop(['url', 'name'], axis=1)\n",
    "\n",
    "# Change the column names to 'negative', 'positive', and 'neutral'\n",
    "plot_day.columns = ['negative', 'positive', 'neutral']\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "plot_day.plot(kind='bar', color=COLORS, figsize=(10,5), width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "import gensim.summarization.bm25 as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = get_bm25_weights(master_file.iloc[0:10]['name'], n_jobs=-1)\n",
    "#gbm.get_scores(master_file.iloc[0:1]['name'],[1])\n",
    "Lexicon= pd.read_csv('OwnLexiconPolarity.csv')\n",
    "\n",
    "\n",
    "a= bm.BM25(master_file.iloc[0:20192]['name'])\n",
    "b=a.get_scores_bow('digest')\n",
    "\n",
    "\n",
    "b[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def addBM25():\n",
    "    super_dict = collections.defaultdict(list)\n",
    "    dic={}\n",
    "    a= bm.BM25(master_file.iloc[0:20192]['name'])\n",
    "    for i in query_tokens[0:955]:\n",
    "        b=a.get_scores_bow(i)\n",
    "        for i in b:\n",
    "            key=i[0]\n",
    "            value=i[1]\n",
    "            dic[key]=value\n",
    "        for k,v in dic.items():\n",
    "            super_dict[k].append(v)\n",
    "    return super_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_result=addBM25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for k, v in end_result.items():\n",
    "    result[k] = sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDataBM25(data,tokens):\n",
    "    data=[word for word in data['name']]\n",
    "    polarity_result=0\n",
    "    result_list=[]\n",
    "    count=0\n",
    "    while count < len(data):\n",
    "        for i in tokens:\n",
    "            temp=a.get_scores(i)\n",
    "            polarity_result+=temp[count]\n",
    "        result_list.append(polarity_result)\n",
    "        polarity_result=0\n",
    "        count+=1\n",
    "    return result_list\n",
    "labelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "s=pd.Series(Lexicon['Word'])\n",
    "query=s.str.cat(sep=' ')\n",
    "query=query.lower()\n",
    "query_tokens=word_tokenize(query)\n",
    "len(master_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def flatten(x):\n",
    "    result = []\n",
    "    for el in x:\n",
    "        if hasattr(el, \"__iter__\") and not isinstance(el, str):\n",
    "            result.extend(flatten(el))\n",
    "        else:\n",
    "            result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=master_file.iloc[0:20000]['name']\n",
    "words=[ word for word in df if len(word)>2]\n",
    "\n",
    "\n",
    "words=flatten(words)\n",
    "\n",
    "top_N = 100\n",
    "word_dist = nltk.FreqDist(words)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRAKTISCHE DURCHFÃHRUNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_LM_lexicon=pd.read_csv(\"LmLexiconPolarityNeu.csv\")\n",
    "df_own_lexicon=pd.read_csv(\"OwnLexiconPolarityNeu.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_lexicon.polarityScore.value_counts(bins=6),df_LM_lexicon.polarityScore.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_own_lexicon['polarityScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM_lexicon['polarityScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "own_dict=dict(zip(df_own_lexicon.Word.str.lower(), df_own_lexicon.polarityScore))\n",
    "own_dict\n",
    "\n",
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "LM_dict=dict(zip(df_LM_lexicon.Word.str.lower(), df_LM_lexicon.polarityScore))\n",
    "LM_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "files = glob.glob(r'//*.csv')\n",
    "\n",
    "df = pd.concat([pd.read_csv(f, index_col=[0,1]) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSentiWordNet(master_file,\"masterFileSentiLexikon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,LM_dict,\"masterFileLmLexikonNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,own_dict,\"masterFileOwnLexikonNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_lm=pd.read_csv(\"masterFileLmLexikonNeu.csv\")\n",
    "#master_file_lm['numMentions'] = master_file_lm['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_lm=master_file_lm.drop('Unnamed: 0',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_own_neu=pd.read_csv(\"masterFileOwnLexikonNeu.csv\")\n",
    "#master_file_own_neu['numMentions'] = master_file_own_neu['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_own_neu=master_file_own_neu.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_senti=pd.read_csv(\"masterFileSentiLexikon.csv\")\n",
    "#master_file_senti['numMentions'] = master_file_senti['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_senti=master_file_senti.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(master_file_lm[master_file_lm['numMentions']>0]),len(master_file_lm[master_file_lm['numMentions']<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(master_file_senti[master_file_senti['numMentions']>0]),len(master_file_senti[master_file_senti['numMentions']<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_file_own_neu[master_file_own_neu['numMentions']>0]),len(master_file_own_neu[master_file_own_neu['numMentions']<0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Use English stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# Make sure we see the full column.\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "master_file['stemmed'] = master_file.iloc[0:34647]['name'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "master_file\n",
    "\n",
    "\n",
    "master_file.to_csv(\"masterFileAllStemmedNeu.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file.to_csv(\"masterFileAllStemmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LM-Dictionary has float values as keys, so we have to delete them\n",
    "for k,w in LM_dict.items():\n",
    "    if isinstance(k,(float)):\n",
    "        LM_dict.pop(k,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LM_dict_stemmed = {stemmer.stem(w) : LM_dict[w] for w in LM_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "own_df_stemmed=pd.DataFrame.from_dict(own_dict_stemmed, orient='index')\n",
    "LM_df_stemmed=pd.DataFrame.from_dict(LM_dict_stemmed, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_df_stemmed.to_csv(\"OwnLexiconPolarityNeuStemmed.csv\")\n",
    "LM_df_stemmed.to_csv(\"LmLexiconPolarityStemmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed=pd.read_csv(\"masterFileAllStemmedNeuNeu.csv\")\n",
    "master_file_stemmed['numMentions'] = master_file_stemmed['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_stemmed=master_file_stemmed.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_df_stemmed=pd.read_csv(\"OwnLexiconPolarityNeuStemmed.csv\")\n",
    "LM_df_stemmed=pd.read_csv(\"LmLexiconPolarityStemmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_df_stemmed = own_df_stemmed.rename(columns={'Unnamed: 0':'Word', '0': 'polarityScore'})\n",
    "LM_df_stemmed=LM_df_stemmed.rename(columns={'Unnamed: 0':'Word', '0': 'polarityScore'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "own_dic_stemmed=dict(zip(own_df_stemmed.Word.str.lower(), own_df_stemmed.polarityScore))\n",
    "own_dic_stemmed\n",
    "\n",
    "#Create a new dic with the necessary columns for sentiment analysis\n",
    "LM_dic_stemmed=dict(zip(LM_df_stemmed.Word.str.lower(), LM_df_stemmed.polarityScore))\n",
    "LM_dic_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label all rows in a DataFrame into negative/positive DataFrames\n",
    "def labelDataSet(data,dictionary,pathTo):\n",
    "    row_counter=0\n",
    "    polarity_result=0\n",
    "    pol_list=[]\n",
    "    while row_counter <len(data['stemmed']): \n",
    "        for i in data['stemmed'][row_counter]:\n",
    "            if dictionary.get(i) is not None:\n",
    "                pol_list.append(dictionary.get(i))\n",
    "                polarity_result=sum(pol_list)/len(pol_list)\n",
    "                data['numMentions'][row_counter]=polarity_result\n",
    "        \n",
    "        row_counter+=1\n",
    "    return data.to_csv(pathTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file_stemmed,LM_dic_stemmed,\"masterFileLMStemmedNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_own=pd.read_csv(\"masterFileOwnStemmed.csv\")\n",
    "master_file_stemmed_own['numMentions'] = master_file_stemmed_own['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_stemmed_own=master_file_stemmed_own.drop('Unnamed: 0',axis=1)\n",
    "master_file_stemmed_own['date'] = pd.to_datetime(master_file_stemmed_own.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_LM_test=pd.read_csv(\"masterFileLMStemmed.csv\")\n",
    "#master_file_stemmed_LM['numMentions'] = master_file_stemmed_LM['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_stemmed_LM=master_file_stemmed_LM.drop('Unnamed: 0',axis=1)\n",
    "master_file_stemmed_LM['date'] = pd.to_datetime(master_file_stemmed_LM.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('amazon')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_LM.to_csv(\"masterFileLMStemmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('amazon')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_df_stemmed.polarityScore.value_counts(bins=3),own_df_stemmed.polarityScore.value_counts(bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#own-lexikon-stemmed\n",
    "stemmed_amazon=master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('amazon')].groupby(['date']).sum()\n",
    "stemmed_google=master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('google')].groupby(['date']).sum()\n",
    "stemmed_facebook=master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('facebook')].groupby(['date']).sum()\n",
    "stemmed_apple=master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('apple')].groupby(['date']).sum()\n",
    "stemmed_microsoft=master_file_stemmed_own[master_file_stemmed_own['stemmed'].str.contains('microsoft')].groupby(['date']).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm-lexikon-stemmed\n",
    "stemmed_amazon_lm=master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('amazon')].groupby(['date']).sum()\n",
    "stemmed_google_lm=master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('google')].groupby(['date']).sum()\n",
    "stemmed_facebook_lm=stemmed_amazon_lm=master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('facebook')].groupby(['date']).sum()\n",
    "stemmed_apple_lm=stemmed_amazon_lm=master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('apple')].groupby(['date']).sum()\n",
    "stemmed_microsoft_lm=stemmed_amazon_lm=master_file_stemmed_LM[master_file_stemmed_LM['stemmed'].str.contains('microsoft')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stock-data\n",
    "\n",
    "apple_data=pd.read_csv('AAPL.csv')\n",
    "apple_data['endResult']=apple_data['Close']-apple_data['Open']\n",
    "\n",
    "amazon_data=pd.read_csv('AMZN.csv')\n",
    "amazon_data['endResult']=amazon_data['Close']-amazon_data['Open']\n",
    "\n",
    "fb_data=pd.read_csv('FB.csv')\n",
    "fb_data['endResult']=fb_data['Close']-fb_data['Open']\n",
    "\n",
    "google_data=pd.read_csv('GOOG.csv')\n",
    "google_data['endResult']=google_data['Close']-google_data['Open']\n",
    "\n",
    "microsoft_data=pd.read_csv('MSFT.csv')\n",
    "microsoft_data['endResult']=microsoft_data['Close']-microsoft_data['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock-data to excel\n",
    "with pd.ExcelWriter('PredictionsStemmed.xlsx') as writer:  \n",
    "    apple_data[['Date','endResult']].to_excel(writer, sheet_name='AppleStemmed')\n",
    "    amazon_data[['Date','endResult']].to_excel(writer, sheet_name='AmazonStemmed')\n",
    "    fb_data[['Date','endResult']].to_excel(writer, sheet_name='FbStemmed')\n",
    "    google_data[['Date','endResult']].to_excel(writer, sheet_name='GoogleStemmed')\n",
    "    microsoft_data[['Date','endResult']].to_excel(writer, sheet_name='MicrosoftStemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelDataSet(master_file,LM_dic_stemmed,\"masterFileLMStemmedNeuNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_own=pd.read_csv(\"masterFileOwnStemmedNeu.csv\")\n",
    "master_file_stemmed_own['numMentions'] = master_file_stemmed_own['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_stemmed_own=master_file_stemmed_own.drop('Unnamed: 0',axis=1)\n",
    "master_file_stemmed_own['date'] = pd.to_datetime(master_file_stemmed_own.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_LM=pd.read_csv(\"masterFileLMStemmedNeuNeu.csv\")\n",
    "master_file_stemmed_LM['numMentions'] = master_file_stemmed_LM['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_stemmed_LM=master_file_stemmed_LM.drop('Unnamed: 0',axis=1)\n",
    "master_file_stemmed_LM['date'] = pd.to_datetime(master_file_stemmed_LM.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_stemmed_LM[master_file_stemmed_LM['numMentions']<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET OF WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file['union'] = master_file.iloc[0:2]['name'].apply(set(testing).union(set(testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file['NameSet'] = master_file.apply(lambda row: set(row['name']), axis=1)\n",
    "master_file['StemmedSet'] = master_file.apply(lambda row: set(row['stemmed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file.to_csv(\"masterFileSetUnion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_union=pd.read_csv(\"masterFileSetUnion.csv\")\n",
    "master_file_union['numMentions'] = master_file_union['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_union=master_file_union.drop('Unnamed: 0',axis=1)\n",
    "master_file_union['date'] = pd.to_datetime(master_file_union.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,own_dict,\"masterFileOwnSetUnionNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,LM_dict,\"masterFileLMSetUnionNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSentiWordNet(master_file,\"masterFileSentiSetUnionNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_set=pd.read_csv(\"masterFileOwnSetUnionNeu.csv\")\n",
    "master_file_set['numMentions'] = master_file_set['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_set=master_file_set.drop('Unnamed: 0',axis=1)\n",
    "master_file_set['date'] = pd.to_datetime(master_file_set.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_set_LM=pd.read_csv(\"masterFileLMSetUnionNeu.csv\")\n",
    "master_file_set_LM['numMentions'] = master_file_set_LM['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_set_LM=master_file_set_LM.drop('Unnamed: 0',axis=1)\n",
    "master_file_set_LM['date'] = pd.to_datetime(master_file_set_LM.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_set_senti=pd.read_csv(\"masterFileSentiSetUnionNeu.csv\")\n",
    "#master_file_set_senti['numMentions'] = master_file_set_senti['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_set_senti=master_file_set_senti.drop('Unnamed: 0',axis=1)\n",
    "master_file_set_senti['date'] = pd.to_datetime(master_file_set_senti.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#own-lexikon\n",
    "amazon_set_own=master_file_set[master_file_set['NameSet'].str.contains('amazon')].groupby(['date']).sum()\n",
    "apple_set_own=master_file_set[master_file_set['NameSet'].str.contains('apple')].groupby(['date']).sum()\n",
    "google_set_own=master_file_set[master_file_set['NameSet'].str.contains('google')].groupby(['date']).sum()\n",
    "microsoft_set_own=master_file_set[master_file_set['NameSet'].str.contains('microsoft')].groupby(['date']).sum()\n",
    "fb_set_own=master_file_set[master_file_set['NameSet'].str.contains('facebook')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM-Lexikon\n",
    "amazon_set_lm=master_file_set_LM[master_file_set_LM['NameSet'].str.contains('amazon')].groupby(['date']).sum()\n",
    "apple_set_lm=master_file_set_LM[master_file_set_LM['NameSet'].str.contains('apple')].groupby(['date']).sum()\n",
    "google_set_lm=master_file_set_LM[master_file_set_LM['NameSet'].str.contains('google')].groupby(['date']).sum()\n",
    "microsoft_set_lm=master_file_set_LM[master_file_set_LM['NameSet'].str.contains('microsoft')].groupby(['date']).sum()\n",
    "fb_set_lm=master_file_set_LM[master_file_set_LM['NameSet'].str.contains('amazon')].groupby(['date']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiwordnet lexikon\n",
    "amazon_set_senti=master_file_set_senti[master_file_set_senti['NameSet'].str.contains('amazon')].groupby(['date']).sum()\n",
    "apple_set_senti=master_file_set_senti[master_file_set_senti['NameSet'].str.contains('apple')].groupby(['date']).sum()\n",
    "google_set_senti=master_file_set_senti[master_file_set_senti['NameSet'].str.contains('google')].groupby(['date']).sum()\n",
    "microsoft_set_senti=master_file_set_senti[master_file_set_senti['NameSet'].str.contains('microsoft')].groupby(['date']).sum()\n",
    "fb_set_senti=master_file_set_senti[master_file_set_senti['NameSet'].str.contains('amazon')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple_set_senti.groupby(['date']).sum()\n",
    "#amazon_set_senti.groupby(['date']).sum()\n",
    "#google_set_senti.groupby(['date']).sum()\n",
    "#microsoft_set_senti.groupby(['date']).sum()\n",
    "fb_set_senti.groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock-data to excel\n",
    "with pd.ExcelWriter('PredictionsSetUnion.xlsx') as writer:  \n",
    "    apple_data[['Date','endResult']].to_excel(writer, sheet_name='AppleSet')\n",
    "    amazon_data[['Date','endResult']].to_excel(writer, sheet_name='AmazonSet')\n",
    "    fb_data[['Date','endResult']].to_excel(writer, sheet_name='FbSet')\n",
    "    google_data[['Date','endResult']].to_excel(writer, sheet_name='GoogleSet')\n",
    "    microsoft_data[['Date','endResult']].to_excel(writer, sheet_name='MicrosoftSet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Of words + Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,own_dict,\"masterFileOwnSetUnionStemmedNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDataSet(master_file,LM_dict,\"masterFileLMSetUnionStemmedNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_set_stem=pd.read_csv(\"masterFileOwnSetUnionStemmedNeu.csv\")\n",
    "master_file_set_stem['numMentions'] = master_file_set_stem['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_set_stem=master_file_set_stem.drop('Unnamed: 0',axis=1)\n",
    "master_file_set_stem['date'] = pd.to_datetime(master_file_set_stem.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_set_stem_LM=pd.read_csv(\"masterFileLMSetUnionStemmedNeu.csv\")\n",
    "master_file_set_stem_LM['numMentions'] = master_file_set_stem_LM['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "master_file_set_stem_LM=master_file_set_stem_LM.drop('Unnamed: 0',axis=1)\n",
    "master_file_set_stem_LM['date'] = pd.to_datetime(master_file_set_stem_LM.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock-data to excel\n",
    "with pd.ExcelWriter('PredictionsSetUnionStemmed.xlsx') as writer:  \n",
    "    apple_data[['Date','endResult']].to_excel(writer, sheet_name='AppleSetStem')\n",
    "    amazon_data[['Date','endResult']].to_excel(writer, sheet_name='AmazonSetStem')\n",
    "    fb_data[['Date','endResult']].to_excel(writer, sheet_name='FbSetStem')\n",
    "    google_data[['Date','endResult']].to_excel(writer, sheet_name='GoogleSetStem')\n",
    "    microsoft_data[['Date','endResult']].to_excel(writer, sheet_name='MicrosoftSetStem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#own-lexikon\n",
    "amazon_set_stem_own=master_file_set_stem[master_file_set_stem['StemmedSet'].str.contains('amazon')].groupby(['date']).sum()\n",
    "apple_set_stem_own=master_file_set_stem[master_file_set_stem['StemmedSet'].str.contains('apple')].groupby(['date']).sum()\n",
    "fb_set_stem_own=master_file_set_stem[master_file_set_stem['StemmedSet'].str.contains('facebook')].groupby(['date']).sum()\n",
    "google_set_stem_own=master_file_set_stem[master_file_set_stem['StemmedSet'].str.contains('google')].groupby(['date']).sum()\n",
    "microsoft_set_stem_own=master_file_set_stem[master_file_set_stem['StemmedSet'].str.contains('microsoft')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LM-Lexikon\n",
    "amazon_set_stem_lm=master_file_set_stem_LM[master_file_set_stem_LM['StemmedSet'].str.contains('amazon')].groupby(['date']).sum()\n",
    "apple_set_stem_lm=master_file_set_stem_LM[master_file_set_stem_LM['StemmedSet'].str.contains('apple')].groupby(['date']).sum()\n",
    "fb_set_stem_lm=master_file_set_stem_LM[master_file_set_stem_LM['StemmedSet'].str.contains('facebook')].groupby(['date']).sum()\n",
    "google_set_stem_lm=master_file_set_stem_LM[master_file_set_stem_LM['StemmedSet'].str.contains('google')].groupby(['date']).sum()\n",
    "microsoft_set_stem_lm=master_file_set_stem_LM[master_file_set_stem_LM['StemmedSet'].str.contains('microsoft')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#microsoft_set_stem_own.groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM 25 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import get_bm25_weights\n",
    "import gensim.summarization.bm25 as bm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LM_lexicon=pd.read_csv(\"LmLexiconPolarityNeu.csv\")\n",
    "df_own_lexicon=pd.read_csv(\"OwnLexiconPolarityNeu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = get_bm25_weights(master_file.iloc[0:10]['name'], n_jobs=-1)\n",
    "#gbm.get_scores(master_file.iloc[0:1]['name'],[1])\n",
    "Lexicon= pd.read_csv('OwnLexiconPolarity.csv')\n",
    "\n",
    "\n",
    "a= bm.BM25(master_file_set_stem.iloc[0:34648]['StemmedSet'])\n",
    "#b=a.get_scores_bow('digest')\n",
    "\n",
    "\n",
    "#b[0]\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "s=pd.Series(own_df_stemmed['Word'])\n",
    "query=s.str.cat(sep=' ')\n",
    "query=query.lower()\n",
    "query_tokens=word_tokenize(query)\n",
    "len(query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def addBM25():\n",
    "    super_dict = collections.defaultdict(list)\n",
    "    dic={}\n",
    "    a= bm.BM25(master_file_set_stem.iloc[0:34648]['StemmedSet'])\n",
    "    for i in query_tokens[0:947]:\n",
    "        b=a.get_scores_bow(i)\n",
    "        for i in b:\n",
    "            key=i[0]\n",
    "            value=i[1]\n",
    "            dic[key]=value\n",
    "        for k,v in dic.items():\n",
    "            super_dict[k].append(v)\n",
    "    return super_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end_result=addBM25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = {}\n",
    "for k, v in end_result.items():\n",
    "    result[k] = sum(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "masterFile_BM25_freq_own=master_file_set_stem\n",
    "masterFile_BM25_freq_own[\"BM25\"] = pd.Series(result)\n",
    "#masterFile_BM25_freq_own=masterFile_BM25_freq_own.drop(columns='D',axis=1)\n",
    "masterFile_BM25_freq_own=masterFile_BM25_freq_own.fillna(0)\n",
    "masterFile_BM25_freq_own[masterFile_BM25_freq_own['numMentions']>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "masterFileBm25=pd.read_csv(\"masterFileBM25SetOwn.csv\")\n",
    "#masterFileBm25['numMentions'] = masterFileBm25['numMentions'].apply(lambda x: float(x.split()[0].replace(',', '').replace('[',' ')))\n",
    "masterFileBm25=masterFileBm25.drop('Unnamed: 0',axis=1)\n",
    "masterFileBm25['date'] = pd.to_datetime(masterFileBm25.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algebraic sign of the Values that are negative in numMentions columns are being reflected on BM25 column values\n",
    "masterFileBm25.loc[master_file_set_stem.numMentions<0,'BM25'] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterFileBm25[masterFileBm25['NameSet'].str.contains('microsoft')].groupby(['date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock-data to excel\n",
    "with pd.ExcelWriter('PredictionsBM25SetStemmed.xlsx') as writer:  \n",
    "    apple_data[['Date','endResult']].to_excel(writer, sheet_name='AppleSetStemmed')\n",
    "    amazon_data[['Date','endResult']].to_excel(writer, sheet_name='AmazonSetStemmed')\n",
    "    fb_data[['Date','endResult']].to_excel(writer, sheet_name='FbSetSetStemmed')\n",
    "    google_data[['Date','endResult']].to_excel(writer, sheet_name='GoogleSetStemmed')\n",
    "    microsoft_data[['Date','endResult']].to_excel(writer, sheet_name='MicrosoftSetStemmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_file_stemmed_LM[master_file_stemmed_LM['numMentions']>0]),len(master_file_stemmed_LM[master_file_stemmed_LM['numMentions']<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PredictionFreq Modell\n",
    "AppleBm25LM=10/20\n",
    "AppleBM25Own=12/20\n",
    "AppleOwn=14/20\n",
    "AppleLM=14/20\n",
    "AppleSenti=11/20\n",
    "\n",
    "GoogleLMBM25=11/20\n",
    "GoogleBM25Own=11/20\n",
    "GoogleLM=11/20\n",
    "GoogleOwn=11/20\n",
    "GoogleSenti=10/20\n",
    "\n",
    "amazonbmLM=11/20\n",
    "amazonbmOwn=12/20\n",
    "AmazonLM=11/20\n",
    "AmazonOwn=12/20\n",
    "AmazonSenti=12/20\n",
    "\n",
    "microsoftbmLM=9/20\n",
    "microsftbmOwn=12/20\n",
    "MicrosoftLM=15/20\n",
    "MicrosoftOwn= 12/20\n",
    "MicrosoftSenti=11/20\n",
    "\n",
    "fbbmLM=10/20\n",
    "fbbmown=11/20\n",
    "facebookLM=8/20\n",
    "facebookOwn=12/20\n",
    "facebookSenti=10/20\n",
    "\n",
    "#PredictionFreqStemmed\n",
    "applebmLM=12/20\n",
    "applebmOwn=11/20\n",
    "AppleLM=12/20\n",
    "AppleOwn=12/20\n",
    "\n",
    "amazonbmLM=10/20\n",
    "amazonbmOwn=9/20\n",
    "AmazonLM=11/20\n",
    "AmazonOwn=11/20\n",
    "\n",
    "fbBmLM=10/20\n",
    "fbBmOwn=9/20\n",
    "FbLM=11/20\n",
    "FbOwn=10/20\n",
    "\n",
    "googlebmLM=9/20\n",
    "googlebmOwn=10/20\n",
    "GoogleLM=11/20\n",
    "GoogleOwn=11/20\n",
    "\n",
    "MicrosoftbmLM=10/20\n",
    "microssftbmOwn=11/20\n",
    "MicrosoftLM=11/20\n",
    "MicrosoftOwn=11/20\n",
    "\n",
    "#PredictionSet\n",
    "applebmLM=11/20\n",
    "applebmOwn=9/20\n",
    "AppleLM=9/20\n",
    "AppleOwn=11/20\n",
    "AppleSenti=11/20\n",
    "\n",
    "amazonbmLM=9/20\n",
    "amazonbmOwn=9/120\n",
    "AmazonLM=13/20\n",
    "AmazonOwn=11/20\n",
    "AmazonSenti=11/20\n",
    "\n",
    "FbbmLM=9/20\n",
    "FbbmOwn=9/20\n",
    "FbLM=7/20\n",
    "FbOwn=9/20\n",
    "FbSenti=10/20\n",
    "\n",
    "GooglebmLM=9/20\n",
    "GooglebmOwn=9/20\n",
    "GoogleLM=11/20\n",
    "GoogleOwn=11/20\n",
    "GoogleSenti=11/20\n",
    "\n",
    "Microsoft=9/10\n",
    "Microsftbmown=9/10\n",
    "MicrosoftLM=15/20\n",
    "MicrosoftOwn=11/20\n",
    "Microsoft=11/20\n",
    "\n",
    "#PredictionSetStemmed\n",
    "\n",
    "ApplebmLM==11/20\n",
    "ApplebmOwn=11/20\n",
    "AppleLM=11/20\n",
    "AppleOwn=12/20\n",
    "\n",
    "AmazonbmLM=10/20\n",
    "AmazonbmOwn=11/20\n",
    "AmazonLM=14/20\n",
    "AmazonOwn=11/20\n",
    "\n",
    "FbBmLM=9/20\n",
    "FbBmOwn=10/20\n",
    "FbLM=10/20\n",
    "FbOwn=10/20\n",
    "\n",
    "googlebmLM=9/10\n",
    "googlebmOwn=11/20\n",
    "GoogleLM=12/20\n",
    "GoogleOwn=11/20\n",
    "\n",
    "microsoftbmLM=10/20\n",
    "microsftbmOwn=11/20\n",
    "MicrosoftLM=16/20\n",
    "MicrosoftOwn=11/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amount of word in our lexicons after stemming it\n",
    "((-1.003, -0.333]    3350\n",
    " (0.333, 1.0]        1277\n",
    " (-0.333, 0.333]        0\n",
    " Name: polarityScore, dtype: int64, (-0.333, 0.333]     364\n",
    " (0.333, 1.0]        313\n",
    " (-1.003, -0.333]    269\n",
    " Name: polarityScore, dtype: int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_df_stemmed['polarityScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news.loc[(news.positivity >3) | (news.sentiment >0)]),len(news.loc[(news.positivity <3) | (news.sentiment <0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "text=' '.join(tokens)\n",
    "\n",
    "# Generate a word cloud image\n",
    "stop_words = [\"google\", \"amazon\", \"apple\", \"facebook\"] + list(STOPWORDS)\n",
    "STOPWORDS.update(ignore_words)\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\",width=1920, height=1080, max_words=100).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=microsoft_data['name'][0:len(microsoft_data)]\n",
    "words=[''.join(word) for word in words]\n",
    "words=''.join(words)\n",
    "tokens=word_tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_data=master_file_union[master_file_union['name'].str.contains('microsoft')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
